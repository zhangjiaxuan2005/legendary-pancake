{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 词袋模型",
   "id": "dfc2e23aa0232e40"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-09T07:23:58.108103Z",
     "start_time": "2025-07-09T07:23:58.098312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "texts = [\n",
    "    \"I love natural language processing.\",\n",
    "    \"I love machine learning.\",\n",
    "    \"I love coding in Python and Java.\",\n",
    "    \"I love Java.\",\n",
    "    \"I love Java, I don't love C++.\",\n",
    "    \"I don't love Java.\"\n",
    "]"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T06:59:56.946663Z",
     "start_time": "2025-07-09T06:59:56.940113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = [word for text in texts for word in text.split()]\n",
    "print(words)"
   ],
   "id": "d90d19bde149dfc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'natural', 'language', 'processing.', 'I', 'love', 'machine', 'learning.', 'I', 'love', 'coding', 'in', 'Python', 'and', 'Java.', 'I', 'love', 'Java.', 'I', 'love', 'Java,', 'I', \"don't\", 'love', 'C++.', 'I', \"don't\", 'love', 'Java.']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T06:59:58.507790Z",
     "start_time": "2025-07-09T06:59:58.499055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary = {}\n",
    "for word in words:\n",
    "    if word not in vocabulary:\n",
    "        vocabulary[word] = len(vocabulary)\n",
    "\n",
    "vocabulary"
   ],
   "id": "ed00c50ad0e2f2cc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 0,\n",
       " 'love': 1,\n",
       " 'natural': 2,\n",
       " 'language': 3,\n",
       " 'processing.': 4,\n",
       " 'machine': 5,\n",
       " 'learning.': 6,\n",
       " 'coding': 7,\n",
       " 'in': 8,\n",
       " 'Python': 9,\n",
       " 'and': 10,\n",
       " 'Java.': 11,\n",
       " 'Java,': 12,\n",
       " \"don't\": 13,\n",
       " 'C++.': 14}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bows = []\n",
    "for text in texts:\n",
    "    bow = [0] * len(vocabulary)\n",
    "    for word in text.split():\n",
    "        bow[vocabulary[word]] += 1\n",
    "    bows.append(bow)\n",
    "\n",
    "bows"
   ],
   "id": "9ba58e6dec3a076",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 相似度",
   "id": "e4572f397c9d6c27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(v1 * v2 for v1, v2 in zip(vector1, vector2))\n",
    "    magnitude1 = sum(v ** 2 for v in vector1) ** 0.5\n",
    "    magnitude2 = sum(v ** 2 for v in vector2) ** 0.5\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "cosine_similarity(bows[0], bows[1])"
   ],
   "id": "1995457c71d0d15c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## skip-gram",
   "id": "bea925b98e9dd5bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sentences = [text.split() for text in texts]\n",
    "sentences"
   ],
   "id": "feca84aa2fa8d575",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vocabulary = {}\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(vocabulary)\n",
    "vocabulary"
   ],
   "id": "630c4d11c0ceef41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### One-hot 编码",
   "id": "2e1aa458f006c0ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def one_hot_encoding(index, vocab_size):\n",
    "    one_hot = torch.zeros(vocab_size)\n",
    "    one_hot[index] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "one_hot_encoding(vocabulary['I'], len(vocabulary))"
   ],
   "id": "f1b8f9da9f8ef47c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def generate_training_data(sentences, window_size):\n",
    "    center_words = []\n",
    "    target_words = []\n",
    "    for sentence in sentences:\n",
    "        indices = [vocabulary[word] for word in sentence]\n",
    "\n",
    "        for center_index in range(len(indices)):\n",
    "            start = max(0, center_index - window_size)\n",
    "            end = min(len(indices), center_index + window_size + 1)\n",
    "\n",
    "            for context_index in range(start, end):\n",
    "                if context_index != center_index:\n",
    "                    center_words.append(indices[center_index])\n",
    "                    target_words.append(indices[context_index])\n",
    "\n",
    "    return torch.tensor(center_words), torch.tensor(target_words)\n",
    "\n",
    "\n",
    "widow_size = 2\n",
    "center_words, target_words = generate_training_data(sentences, widow_size)\n",
    "center_words, target_words"
   ],
   "id": "b7729900c2d41b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(center_words, target_words)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ],
   "id": "3f739f06b94f2e02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 构建模型",
   "id": "268d7e5d6f7ed63d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.input_embeddings = nn.Linear(vocab_size, embedding_dim, bias=False)\n",
    "        self.output_embeddings = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, center_word):\n",
    "        center_word_onehot = one_hot_encoding(center_word, vocab_size).view(1, -1)\n",
    "        hidden = self.input_embeddings(center_word_onehot)\n",
    "        return self.output_embeddings(hidden)\n",
    "\n",
    "\n",
    "embedding_dim = 10\n",
    "vocab_size = len(vocabulary)\n",
    "model = SkipGram(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for center_words, target_words in dataloader:\n",
    "        output = model(center_words)\n",
    "        loss = criterion(output, target_words)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch + 1}/{epochs}, Loss: {loss.item()}\")"
   ],
   "id": "c40cc4241978872a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "word_vectors = model.input_embeddings.weight.T\n",
    "print(word_vectors.shape)"
   ],
   "id": "8c3d9176524d077e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for word, index in vocabulary.items():\n",
    "    print(f\"{word}: {word_vectors[index]}\")"
   ],
   "id": "a9470e1cf7331fce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def encode_sentence(sentence):\n",
    "    vector = [word_vectors[:, vocabulary[word]].detach().numpy() for word in sentence.split()]\n",
    "    print(vector)\n",
    "    return np.mean(vector, axis=0)\n",
    "\n",
    "\n",
    "encode_sentence(\"I love machine learning.\")\n"
   ],
   "id": "76e738cdbc7e31f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 嵌入层优化",
   "id": "ddda72d1468473d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.input_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output_embeddings = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, center_word):\n",
    "        hidden = self.input_embeddings(center_word)\n",
    "        return self.output_embeddings(hidden)\n",
    "\n",
    "\n",
    "embedding_dim = 10\n",
    "vocab_size = len(vocabulary)\n",
    "model = SkipGram(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for center_words, target_words in dataloader:\n",
    "        output = model(center_words)\n",
    "        loss = criterion(output, target_words)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch + 1}/{epochs}, Loss: {loss.item()}\")"
   ],
   "id": "d2041d12039324a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "word_vectors = model.input_embeddings.weight.T\n",
    "for word, index in vocabulary.items():\n",
    "    print(f\"{word}: {word_vectors[index]}\")"
   ],
   "id": "5e5669b3310cd861",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cbow",
   "id": "64d24ae766ece292"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T06:59:17.224930Z",
     "start_time": "2025-07-09T06:59:17.215358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = [text.split() for text in texts]\n",
    "sentences\n"
   ],
   "id": "5b87572b94fd3546",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'love', 'natural', 'language', 'processing.'],\n",
       " ['I', 'love', 'machine', 'learning.'],\n",
       " ['I', 'love', 'coding', 'in', 'Python', 'and', 'Java.'],\n",
       " ['I', 'love', 'Java.'],\n",
       " ['I', 'love', 'Java,', 'I', \"don't\", 'love', 'C++.'],\n",
       " ['I', \"don't\", 'love', 'Java.']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:01:36.172643Z",
     "start_time": "2025-07-09T07:01:36.164759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary = {}\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(vocabulary)\n",
    "vocabulary"
   ],
   "id": "528602208e9b804c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 0,\n",
       " 'love': 1,\n",
       " 'natural': 2,\n",
       " 'language': 3,\n",
       " 'processing.': 4,\n",
       " 'machine': 5,\n",
       " 'learning.': 6,\n",
       " 'coding': 7,\n",
       " 'in': 8,\n",
       " 'Python': 9,\n",
       " 'and': 10,\n",
       " 'Java.': 11,\n",
       " 'Java,': 12,\n",
       " \"don't\": 13,\n",
       " 'C++.': 14}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:01:46.563449Z",
     "start_time": "2025-07-09T07:01:46.547349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_training_data(sentences, window_size):\n",
    "    context_words = []\n",
    "    target_words = []\n",
    "    for sentence in sentences:\n",
    "        indices = [vocabulary[word] for word in sentence]\n",
    "\n",
    "        for center_index in range(len(indices)):\n",
    "            start = max(0, center_index - window_size)\n",
    "            end = min(len(indices), center_index + window_size + 1)\n",
    "            context=[]\n",
    "\n",
    "            for context_index in range(start, end):\n",
    "                if context_index != center_index:\n",
    "                    context.append(indices[context_index])\n",
    "\n",
    "                if len( context)>=1:\n",
    "                    context_words.append(context)\n",
    "                    target_words.append(indices[center_index])\n",
    "\n",
    "    return context_words, target_words\n",
    "\n",
    "window_size = 2\n",
    "context_words, target_words = generate_training_data(sentences, window_size)\n",
    "context_words, target_words"
   ],
   "id": "4fa7b29c7d8e0beb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 2],\n",
       "  [1, 2],\n",
       "  [0, 2, 3],\n",
       "  [0, 2, 3],\n",
       "  [0, 2, 3],\n",
       "  [0, 2, 3],\n",
       "  [0, 1, 3, 4],\n",
       "  [0, 1, 3, 4],\n",
       "  [0, 1, 3, 4],\n",
       "  [0, 1, 3, 4],\n",
       "  [0, 1, 3, 4],\n",
       "  [1, 2, 4],\n",
       "  [1, 2, 4],\n",
       "  [1, 2, 4],\n",
       "  [1, 2, 4],\n",
       "  [2, 3],\n",
       "  [2, 3],\n",
       "  [2, 3],\n",
       "  [1, 5],\n",
       "  [1, 5],\n",
       "  [0, 5, 6],\n",
       "  [0, 5, 6],\n",
       "  [0, 5, 6],\n",
       "  [0, 5, 6],\n",
       "  [0, 1, 6],\n",
       "  [0, 1, 6],\n",
       "  [0, 1, 6],\n",
       "  [0, 1, 6],\n",
       "  [1, 5],\n",
       "  [1, 5],\n",
       "  [1, 5],\n",
       "  [1, 7],\n",
       "  [1, 7],\n",
       "  [0, 7, 8],\n",
       "  [0, 7, 8],\n",
       "  [0, 7, 8],\n",
       "  [0, 7, 8],\n",
       "  [0, 1, 8, 9],\n",
       "  [0, 1, 8, 9],\n",
       "  [0, 1, 8, 9],\n",
       "  [0, 1, 8, 9],\n",
       "  [0, 1, 8, 9],\n",
       "  [1, 7, 9, 10],\n",
       "  [1, 7, 9, 10],\n",
       "  [1, 7, 9, 10],\n",
       "  [1, 7, 9, 10],\n",
       "  [1, 7, 9, 10],\n",
       "  [7, 8, 10, 11],\n",
       "  [7, 8, 10, 11],\n",
       "  [7, 8, 10, 11],\n",
       "  [7, 8, 10, 11],\n",
       "  [7, 8, 10, 11],\n",
       "  [8, 9, 11],\n",
       "  [8, 9, 11],\n",
       "  [8, 9, 11],\n",
       "  [8, 9, 11],\n",
       "  [9, 10],\n",
       "  [9, 10],\n",
       "  [9, 10],\n",
       "  [1, 11],\n",
       "  [1, 11],\n",
       "  [0, 11],\n",
       "  [0, 11],\n",
       "  [0, 11],\n",
       "  [0, 1],\n",
       "  [0, 1],\n",
       "  [0, 1],\n",
       "  [1, 12],\n",
       "  [1, 12],\n",
       "  [0, 12, 0],\n",
       "  [0, 12, 0],\n",
       "  [0, 12, 0],\n",
       "  [0, 12, 0],\n",
       "  [0, 1, 0, 13],\n",
       "  [0, 1, 0, 13],\n",
       "  [0, 1, 0, 13],\n",
       "  [0, 1, 0, 13],\n",
       "  [0, 1, 0, 13],\n",
       "  [1, 12, 13, 1],\n",
       "  [1, 12, 13, 1],\n",
       "  [1, 12, 13, 1],\n",
       "  [1, 12, 13, 1],\n",
       "  [1, 12, 13, 1],\n",
       "  [12, 0, 1, 14],\n",
       "  [12, 0, 1, 14],\n",
       "  [12, 0, 1, 14],\n",
       "  [12, 0, 1, 14],\n",
       "  [12, 0, 1, 14],\n",
       "  [0, 13, 14],\n",
       "  [0, 13, 14],\n",
       "  [0, 13, 14],\n",
       "  [0, 13, 14],\n",
       "  [13, 1],\n",
       "  [13, 1],\n",
       "  [13, 1],\n",
       "  [13, 1],\n",
       "  [13, 1],\n",
       "  [0, 1, 11],\n",
       "  [0, 1, 11],\n",
       "  [0, 1, 11],\n",
       "  [0, 1, 11],\n",
       "  [0, 13, 11],\n",
       "  [0, 13, 11],\n",
       "  [0, 13, 11],\n",
       "  [0, 13, 11],\n",
       "  [13, 1],\n",
       "  [13, 1],\n",
       "  [13, 1]],\n",
       " [0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  0,\n",
       "  0,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  11,\n",
       "  11,\n",
       "  11])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:01:50.075093Z",
     "start_time": "2025-07-09T07:01:50.063195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_context_size = max(len(context) for context in context_words)\n",
    "padded_contexts = []\n",
    "for context in context_words:\n",
    "    padded=context+[0]*(max_context_size-len(context))\n",
    "    padded_contexts.append(padded)\n",
    "\n",
    "padded_contexts"
   ],
   "id": "512be2cabfe067e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 0, 0],\n",
       " [1, 2, 0, 0],\n",
       " [0, 2, 3, 0],\n",
       " [0, 2, 3, 0],\n",
       " [0, 2, 3, 0],\n",
       " [0, 2, 3, 0],\n",
       " [0, 1, 3, 4],\n",
       " [0, 1, 3, 4],\n",
       " [0, 1, 3, 4],\n",
       " [0, 1, 3, 4],\n",
       " [0, 1, 3, 4],\n",
       " [1, 2, 4, 0],\n",
       " [1, 2, 4, 0],\n",
       " [1, 2, 4, 0],\n",
       " [1, 2, 4, 0],\n",
       " [2, 3, 0, 0],\n",
       " [2, 3, 0, 0],\n",
       " [2, 3, 0, 0],\n",
       " [1, 5, 0, 0],\n",
       " [1, 5, 0, 0],\n",
       " [0, 5, 6, 0],\n",
       " [0, 5, 6, 0],\n",
       " [0, 5, 6, 0],\n",
       " [0, 5, 6, 0],\n",
       " [0, 1, 6, 0],\n",
       " [0, 1, 6, 0],\n",
       " [0, 1, 6, 0],\n",
       " [0, 1, 6, 0],\n",
       " [1, 5, 0, 0],\n",
       " [1, 5, 0, 0],\n",
       " [1, 5, 0, 0],\n",
       " [1, 7, 0, 0],\n",
       " [1, 7, 0, 0],\n",
       " [0, 7, 8, 0],\n",
       " [0, 7, 8, 0],\n",
       " [0, 7, 8, 0],\n",
       " [0, 7, 8, 0],\n",
       " [0, 1, 8, 9],\n",
       " [0, 1, 8, 9],\n",
       " [0, 1, 8, 9],\n",
       " [0, 1, 8, 9],\n",
       " [0, 1, 8, 9],\n",
       " [1, 7, 9, 10],\n",
       " [1, 7, 9, 10],\n",
       " [1, 7, 9, 10],\n",
       " [1, 7, 9, 10],\n",
       " [1, 7, 9, 10],\n",
       " [7, 8, 10, 11],\n",
       " [7, 8, 10, 11],\n",
       " [7, 8, 10, 11],\n",
       " [7, 8, 10, 11],\n",
       " [7, 8, 10, 11],\n",
       " [8, 9, 11, 0],\n",
       " [8, 9, 11, 0],\n",
       " [8, 9, 11, 0],\n",
       " [8, 9, 11, 0],\n",
       " [9, 10, 0, 0],\n",
       " [9, 10, 0, 0],\n",
       " [9, 10, 0, 0],\n",
       " [1, 11, 0, 0],\n",
       " [1, 11, 0, 0],\n",
       " [0, 11, 0, 0],\n",
       " [0, 11, 0, 0],\n",
       " [0, 11, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 12, 0, 0],\n",
       " [1, 12, 0, 0],\n",
       " [0, 12, 0, 0],\n",
       " [0, 12, 0, 0],\n",
       " [0, 12, 0, 0],\n",
       " [0, 12, 0, 0],\n",
       " [0, 1, 0, 13],\n",
       " [0, 1, 0, 13],\n",
       " [0, 1, 0, 13],\n",
       " [0, 1, 0, 13],\n",
       " [0, 1, 0, 13],\n",
       " [1, 12, 13, 1],\n",
       " [1, 12, 13, 1],\n",
       " [1, 12, 13, 1],\n",
       " [1, 12, 13, 1],\n",
       " [1, 12, 13, 1],\n",
       " [12, 0, 1, 14],\n",
       " [12, 0, 1, 14],\n",
       " [12, 0, 1, 14],\n",
       " [12, 0, 1, 14],\n",
       " [12, 0, 1, 14],\n",
       " [0, 13, 14, 0],\n",
       " [0, 13, 14, 0],\n",
       " [0, 13, 14, 0],\n",
       " [0, 13, 14, 0],\n",
       " [13, 1, 0, 0],\n",
       " [13, 1, 0, 0],\n",
       " [13, 1, 0, 0],\n",
       " [13, 1, 0, 0],\n",
       " [13, 1, 0, 0],\n",
       " [0, 1, 11, 0],\n",
       " [0, 1, 11, 0],\n",
       " [0, 1, 11, 0],\n",
       " [0, 1, 11, 0],\n",
       " [0, 13, 11, 0],\n",
       " [0, 13, 11, 0],\n",
       " [0, 13, 11, 0],\n",
       " [0, 13, 11, 0],\n",
       " [13, 1, 0, 0],\n",
       " [13, 1, 0, 0],\n",
       " [13, 1, 0, 0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:01:57.814883Z",
     "start_time": "2025-07-09T07:01:54.974884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(torch.tensor(padded_contexts), torch.tensor(target_words))\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ],
   "id": "ff32310478d49d8b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:02:03.529991Z",
     "start_time": "2025-07-09T07:01:59.892601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.input_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output_embeddings = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_words):\n",
    "        hidden = self.input_embeddings(context_words)\n",
    "        avg=torch.mean(hidden, dim=1)\n",
    "        return self.output_embeddings(avg)\n",
    "\n",
    "embedding_dim=100\n",
    "vocab_size=len(vocabulary)\n",
    "\n",
    "model=CBOW(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for context_words, target_words in dataloader:\n",
    "        output = model(context_words)\n",
    "        loss = criterion(output, target_words)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch + 1}/{epochs}, Loss: {loss.item()}\")"
   ],
   "id": "94aac2bddfc8a328",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 2.883824348449707\n",
      "Epoch: 2/10, Loss: 3.0268871784210205\n",
      "Epoch: 3/10, Loss: 2.508455753326416\n",
      "Epoch: 4/10, Loss: 0.5566090941429138\n",
      "Epoch: 5/10, Loss: 0.48134422302246094\n",
      "Epoch: 6/10, Loss: 0.44803065061569214\n",
      "Epoch: 7/10, Loss: 1.4029945135116577\n",
      "Epoch: 8/10, Loss: 0.2548027038574219\n",
      "Epoch: 9/10, Loss: 1.8012924194335938\n",
      "Epoch: 10/10, Loss: 2.1545863151550293\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:02:06.124223Z",
     "start_time": "2025-07-09T07:02:06.107613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_vectors = model.input_embeddings.weight.T\n",
    "for word, index in vocabulary.items():\n",
    "    print(f\"{word}: {word_vectors[index]}\")"
   ],
   "id": "d343f3ee732a3ad0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: tensor([ 1.3686,  0.3975,  0.9415,  0.6726, -1.7063,  0.4028,  0.0643,  1.2155,\n",
      "        -0.1305, -1.9735, -1.1331, -1.2892, -0.0151,  1.2197, -1.6659],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "love: tensor([-1.5445, -0.2746, -0.2169,  0.5176, -0.9260, -0.0592,  0.5421, -0.1473,\n",
      "        -0.5126,  0.1111,  1.2941,  1.8491,  1.7538,  0.1905,  0.6758],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "natural: tensor([ 0.4634, -0.8749, -1.1605, -0.3061,  0.2505, -1.0818,  0.6324, -0.6324,\n",
      "        -0.7074, -0.2094, -0.1390,  1.8150, -1.0685, -0.8418,  2.3039],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "language: tensor([-0.0440,  0.1713,  0.1589,  0.6060,  1.6509,  0.6998,  1.4455, -0.0582,\n",
      "        -0.8986,  1.4521, -0.7155, -2.3919, -1.2118,  1.1085, -1.1462],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "processing.: tensor([-0.6503, -0.1953,  0.9942, -1.7584, -0.7329, -0.6782,  0.4042, -0.0207,\n",
      "         0.5383, -1.6510,  0.2370,  2.1545,  0.7503,  0.8720,  1.3051],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "machine: tensor([-0.1227, -1.7575, -0.0669, -1.1696,  0.0386,  2.7357,  0.0992, -1.1351,\n",
      "        -0.8743,  0.4579, -0.6574, -0.3345, -0.8901,  1.6071, -0.4783],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "learning.: tensor([ 1.3862, -1.2361,  0.4753,  0.3011,  0.8432,  1.1904,  0.9972, -0.9268,\n",
      "         0.4275,  0.7960,  0.7512,  0.4297, -1.6448,  1.6057, -0.8264],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "coding: tensor([-0.9776,  1.1804, -0.5651,  0.0422,  1.4802, -0.8621,  0.4102,  0.9729,\n",
      "         1.8783, -1.1084, -0.1243,  0.0744,  0.3961, -0.5356,  1.0254],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "in: tensor([ 0.4042, -1.5589,  1.1444,  0.4711,  2.1341,  0.1009, -0.8988,  0.8069,\n",
      "        -1.3788,  1.4341,  0.2356, -0.0215,  0.8237,  0.4672, -0.3164],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Python: tensor([ 0.4564, -0.6830,  1.4540,  0.4184, -1.6228, -0.2450, -1.3993,  0.0686,\n",
      "         0.1973,  0.2952, -0.4100, -1.1135,  1.2648, -1.0539, -0.3281],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "and: tensor([ 0.0176,  0.8917,  1.8084, -0.5415, -0.3011,  0.1935,  0.0742, -0.1220,\n",
      "         1.7323,  0.0636, -2.4404,  1.1451,  0.7805,  0.2618,  1.0177],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Java.: tensor([ 0.3355,  0.7119, -1.4568, -0.4963, -1.1436,  0.3303,  0.5046, -2.0778,\n",
      "        -0.0512,  0.5307,  0.0444,  0.2899, -0.5362,  0.7755, -0.9493],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Java,: tensor([ 0.9420,  1.6461, -0.5151, -2.3098,  2.6996, -0.2768,  0.7289, -0.3527,\n",
      "         0.4237, -1.4181, -1.3910,  0.0874, -1.0843, -0.6335,  1.3477],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "don't: tensor([ 0.9773,  0.0793,  1.7323,  0.9533,  0.3183, -3.2251, -1.5600, -0.6486,\n",
      "         0.6193,  0.8865, -0.9146, -1.7228, -0.2305,  1.0277, -1.5517],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "C++.: tensor([-0.2114, -0.9188, -1.6791, -0.3378,  0.6447, -0.9637, -0.3944, -2.6217,\n",
      "        -0.3946, -0.0599,  1.9282, -1.1199, -0.4694, -0.6931, -0.8649],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
