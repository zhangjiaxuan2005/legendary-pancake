{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## N-Gram",
   "id": "89c4684e6a167cc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### jieba 分词",
   "id": "b5db14862303716a"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import jieba\n",
    "\n",
    "text = '我喜欢自然语言处理'\n",
    "\n",
    "seg_list = jieba.cut(text)\n",
    "print(' '.join(seg_list))"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### bert-base-chinese 分词",
   "id": "d0cd5ff59996cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T01:38:53.790816Z",
     "start_time": "2025-07-14T01:38:38.952969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from modelscope import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('trainsz/bert-base-chinese')\n",
    "\n",
    "text = '我喜欢自然语言处理'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ],
   "id": "6412f09fc0f73a23",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17246\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-14 09:38:51,826 - modelscope - WARNING - Repo trainsz/bert-base-chinese not exists on https://www.modelscope.cn, will try on alternative endpoint https://www.modelscope.ai.\n",
      "2025-07-14 09:38:52,500 - modelscope - ERROR - Repo trainsz/bert-base-chinese not exists on either https://www.modelscope.cn or https://www.modelscope.ai\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "<Response [404]>",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmodelscope\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m tokenizer = AutoTokenizer.from_pretrained(\u001B[33m'\u001B[39m\u001B[33mtrainsz/bert-base-chinese\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      5\u001B[39m text = \u001B[33m'\u001B[39m\u001B[33m我喜欢自然语言处理\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m      6\u001B[39m tokens = tokenizer.tokenize(text)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\modelscope\\utils\\hf_util\\patcher.py:288\u001B[39m, in \u001B[36m_patch_pretrained_class.<locals>.get_wrapped_class.<locals>.ClassWrapper.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    284\u001B[39m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[32m    285\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfrom_pretrained\u001B[39m(\u001B[38;5;28mcls\u001B[39m, pretrained_model_name_or_path,\n\u001B[32m    286\u001B[39m                     *model_args, **kwargs):\n\u001B[32m    287\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m file_pattern_context(kwargs, module_class, \u001B[38;5;28mcls\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m288\u001B[39m         model_dir = get_model_dir(pretrained_model_name_or_path,\n\u001B[32m    289\u001B[39m                                   **kwargs)\n\u001B[32m    291\u001B[39m     module_obj = module_class.from_pretrained(\n\u001B[32m    292\u001B[39m         model_dir, *model_args, **kwargs)\n\u001B[32m    294\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m module_class.\u001B[34m__name__\u001B[39m.startswith(\u001B[33m'\u001B[39m\u001B[33mAutoModel\u001B[39m\u001B[33m'\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\modelscope\\utils\\hf_util\\patcher.py:159\u001B[39m, in \u001B[36m_patch_pretrained_class.<locals>.get_model_dir\u001B[39m\u001B[34m(pretrained_model_name_or_path, ignore_file_pattern, allow_file_pattern, **kwargs)\u001B[39m\n\u001B[32m    157\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m file_filter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    158\u001B[39m     allow_file_pattern = file_filter\n\u001B[32m--> \u001B[39m\u001B[32m159\u001B[39m model_dir = snapshot_download(\n\u001B[32m    160\u001B[39m     pretrained_model_name_or_path,\n\u001B[32m    161\u001B[39m     revision=revision,\n\u001B[32m    162\u001B[39m     ignore_file_pattern=ignore_file_pattern,\n\u001B[32m    163\u001B[39m     allow_file_pattern=allow_file_pattern)\n\u001B[32m    164\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m subfolder:\n\u001B[32m    165\u001B[39m     model_dir = os.path.join(model_dir, subfolder)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\modelscope\\hub\\snapshot_download.py:108\u001B[39m, in \u001B[36msnapshot_download\u001B[39m\u001B[34m(model_id, revision, cache_dir, user_agent, local_files_only, cookies, ignore_file_pattern, allow_file_pattern, local_dir, allow_patterns, ignore_patterns, max_workers, repo_id, repo_type)\u001B[39m\n\u001B[32m    105\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m revision \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    106\u001B[39m     revision = DEFAULT_DATASET_REVISION \u001B[38;5;28;01mif\u001B[39;00m repo_type == REPO_TYPE_DATASET \u001B[38;5;28;01melse\u001B[39;00m DEFAULT_MODEL_REVISION\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _snapshot_download(\n\u001B[32m    109\u001B[39m     repo_id,\n\u001B[32m    110\u001B[39m     repo_type=repo_type,\n\u001B[32m    111\u001B[39m     revision=revision,\n\u001B[32m    112\u001B[39m     cache_dir=cache_dir,\n\u001B[32m    113\u001B[39m     user_agent=user_agent,\n\u001B[32m    114\u001B[39m     local_files_only=local_files_only,\n\u001B[32m    115\u001B[39m     cookies=cookies,\n\u001B[32m    116\u001B[39m     ignore_file_pattern=ignore_file_pattern,\n\u001B[32m    117\u001B[39m     allow_file_pattern=allow_file_pattern,\n\u001B[32m    118\u001B[39m     local_dir=local_dir,\n\u001B[32m    119\u001B[39m     ignore_patterns=ignore_patterns,\n\u001B[32m    120\u001B[39m     allow_patterns=allow_patterns,\n\u001B[32m    121\u001B[39m     max_workers=max_workers)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\modelscope\\hub\\snapshot_download.py:254\u001B[39m, in \u001B[36m_snapshot_download\u001B[39m\u001B[34m(repo_id, repo_type, revision, cache_dir, user_agent, local_files_only, cookies, ignore_file_pattern, allow_file_pattern, local_dir, allow_patterns, ignore_patterns, max_workers)\u001B[39m\n\u001B[32m    251\u001B[39m         headers[\u001B[33m'\u001B[39m\u001B[33mx-aliyun-region-id\u001B[39m\u001B[33m'\u001B[39m] = region_id\n\u001B[32m    253\u001B[39m _api = HubApi()\n\u001B[32m--> \u001B[39m\u001B[32m254\u001B[39m endpoint = _api.get_endpoint_for_read(\n\u001B[32m    255\u001B[39m     repo_id=repo_id, repo_type=repo_type)\n\u001B[32m    256\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cookies \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    257\u001B[39m     cookies = ModelScopeConfig.get_cookies()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\modelscope\\hub\\api.py:342\u001B[39m, in \u001B[36mHubApi.get_endpoint_for_read\u001B[39m\u001B[34m(self, repo_id, repo_type)\u001B[39m\n\u001B[32m    339\u001B[39m logger.warning(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mRepo \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m not exists on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprefer_endpoint\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    340\u001B[39m                \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mwill try on alternative endpoint \u001B[39m\u001B[38;5;132;01m{\u001B[39;00malternative_endpoint\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    341\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m342\u001B[39m     \u001B[38;5;28mself\u001B[39m.repo_exists(\n\u001B[32m    343\u001B[39m         repo_id, repo_type=repo_type, endpoint=alternative_endpoint, re_raise=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    344\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m    345\u001B[39m     logger.error(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mRepo \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m not exists on either \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprefer_endpoint\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m or \u001B[39m\u001B[38;5;132;01m{\u001B[39;00malternative_endpoint\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\modelscope\\hub\\api.py:403\u001B[39m, in \u001B[36mHubApi.repo_exists\u001B[39m\u001B[34m(self, repo_id, repo_type, endpoint, re_raise, token)\u001B[39m\n\u001B[32m    401\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m code == \u001B[32m404\u001B[39m:\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m re_raise:\n\u001B[32m--> \u001B[39m\u001B[32m403\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(r)\n\u001B[32m    404\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    405\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[31mHTTPError\u001B[39m: <Response [404]>"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### N-gram 模型",
   "id": "85ba6f7d4e331f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T12:02:31.528529Z",
     "start_time": "2025-07-13T12:02:31.516147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "I love natural language processing.\n",
    "I love machine learning.\n",
    "I love coding in python and java.\n",
    "I love ArkTS.\n",
    "\"\"\"\n",
    "words = [words for words in text.split()]\n",
    "\n",
    "print(words)"
   ],
   "id": "5bfd3eefd05ac45c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'natural', 'language', 'processing.', 'I', 'love', 'machine', 'learning.', 'I', 'love', 'coding', 'in', 'python', 'and', 'java.', 'I', 'love', 'ArkTS.']\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T12:02:34.105565Z",
     "start_time": "2025-07-13T12:02:34.099063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = 3\n",
    "ngrams = [tuple(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
    "print(ngrams)"
   ],
   "id": "81d8ebd4a99992e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'love', 'natural'), ('love', 'natural', 'language'), ('natural', 'language', 'processing.'), ('language', 'processing.', 'I'), ('processing.', 'I', 'love'), ('I', 'love', 'machine'), ('love', 'machine', 'learning.'), ('machine', 'learning.', 'I'), ('learning.', 'I', 'love'), ('I', 'love', 'coding'), ('love', 'coding', 'in'), ('coding', 'in', 'python'), ('in', 'python', 'and'), ('python', 'and', 'java.'), ('and', 'java.', 'I'), ('java.', 'I', 'love'), ('I', 'love', 'ArkTS.')]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T12:02:38.004308Z",
     "start_time": "2025-07-13T12:02:37.995183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "model = defaultdict(lambda: defaultdict(int))\n",
    "for i in range(len(words) - n + 1):\n",
    "    context = tuple(words[i:i + n - 1])\n",
    "    next_word = words[i + n - 1]\n",
    "    model[context][next_word] += 1\n",
    "\n",
    "print(model)"
   ],
   "id": "77d2a02c7bd1b1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x000001F7B0479EE0>, {('I', 'love'): defaultdict(<class 'int'>, {'natural': 1, 'machine': 1, 'coding': 1, 'ArkTS.': 1}), ('love', 'natural'): defaultdict(<class 'int'>, {'language': 1}), ('natural', 'language'): defaultdict(<class 'int'>, {'processing.': 1}), ('language', 'processing.'): defaultdict(<class 'int'>, {'I': 1}), ('processing.', 'I'): defaultdict(<class 'int'>, {'love': 1}), ('love', 'machine'): defaultdict(<class 'int'>, {'learning.': 1}), ('machine', 'learning.'): defaultdict(<class 'int'>, {'I': 1}), ('learning.', 'I'): defaultdict(<class 'int'>, {'love': 1}), ('love', 'coding'): defaultdict(<class 'int'>, {'in': 1}), ('coding', 'in'): defaultdict(<class 'int'>, {'python': 1}), ('in', 'python'): defaultdict(<class 'int'>, {'and': 1}), ('python', 'and'): defaultdict(<class 'int'>, {'java.': 1}), ('and', 'java.'): defaultdict(<class 'int'>, {'I': 1}), ('java.', 'I'): defaultdict(<class 'int'>, {'love': 1})})\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T12:02:45.405465Z",
     "start_time": "2025-07-13T12:02:45.397832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_next_word(model, context):\n",
    "    context=tuple(context)\n",
    "    if context not in model:\n",
    "        return None\n",
    "    next_words = model[context]\n",
    "    return max(next_words.items(), key=lambda x: x[1])[0]\n"
   ],
   "id": "988a266f6880218f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T12:02:48.537436Z",
     "start_time": "2025-07-13T12:02:48.528897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context = ['I', 'love']\n",
    "print(predict_next_word(model, context))"
   ],
   "id": "4664617701f58af4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
